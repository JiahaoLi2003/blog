---
layout:     post
title:      LLM-Planning			# 标题 
subtitle:   CoT, ToT and GoT   #副标题
date:       2023-10-07 				# 时间
author:     JiahaoLi 						# 作者
header-img: img/bolg-background.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Planning
    - LLMs
---

## 📖 Abstract

随着LLM 推理能力不断的被开发，衍生出了一系列的帮助 LLMs 提高reasoning 能力的prompt技巧。

## ⛓️ COT(chain of thought)
![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/COT.png)
- intro
    -  rationale-augmented training and finetuning method : cost a lot
    -  few-shot prompting : works poorly on tasks that require reasoning ability , and do not help improve as the size of LLM scales up

- method
    - 在解决一个复杂的问题时，考虑自己的思维过程，它会把问题分解成许多中间步骤，并在给出最终答案之前解决每个步骤。

- explaination
    - A chain of thought 为模型的行为提供了一个可解释的窗口，显示了它的推理路径。
    - 将问题分解为许多中间步骤，这意味着在执行每个步骤时都会消耗一些计算资源，这比标准提示花费更多的额外计算。

- discussion
    - 累计误差：中间的推导过程的失败会导致最终结果的失败
    - 随机性：由于LMM带有一定随机性，COT像是一次开放性探索，无法保证thought的质量

## 🎄 TOT(tree of thought)
![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/TOT.png)
- intro
    - 如果把任务看作多步决策问题，引入搜索算法是一个很直观的想法
    - 常见的决策算法是树搜索BFS、DFS
- 搜索算法的设计理念：
    - 步骤解耦：怎么把一个任务划分成马尔科夫的状态链
    - 步骤生成：在一个状态中，如果让模型生成下一步决策
    - 状态评估：如何对当前的状态进行评价，进而获取反馈
    - 搜索逻辑：实际执行的搜索算法
  
- discussion
    - 重要的因素在于同时生成多个样本，进行评估后有选择地向下搜索。相比于CoT来说，样本空间大了很多，和CoT-SC比起来也同理，搜索空间大了很多。

## GOT(graph of thought)
![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/GOT.png)
- GOT通过网络推理增强LLM能力的方法。 在GoT中，LLM思想被建模为顶点，而边是这些思想之间的依赖关系。 使用 GoT可以通过构造具有多个传入边的顶点来聚合任意想法。

- discussion
    - 针对不同任务的最佳图结构的设计
    - 成本控制

### 😉今天的分享就到这里啦，下次见咯












