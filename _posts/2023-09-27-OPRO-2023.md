---
layout:     post
title:      OPRO			# 标题 
subtitle:   LARGE LANGUAGE MODELS AS OPTIMIZERS #副标题
date:       2023-09-27 				# 时间
author:     JiahaoLi 						# 作者
header-img: img/bolg-background.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Experience Learning
    - LLMs
    - agent
---

[![pP7MExx.png](https://z1.ax1x.com/2023/09/25/pP7MExx.png)](https://imgse.com/i/pP7MExx)

## 📖 Abstract

- 文章提出Optimization by PROmpting (OPRO)，使用LLM作为优化器，在每个优化步骤中，LLM 根据包含先前生成的解决方案及其值的提示生成新的解决方案，然后对新解决方案进行评估并将其添加到下一个优化步骤的提示中。
- 通过实验证明OPRP优化的最佳prompt在GSM8K和Big-Bench任务上比人工设计的prompt效果更好。

## 👉Intro

Overview

- OPRO优化目标是找到最大化任务准确性的提示。
- 将元提示作为输入，LLM 会生成目标函数的新解决方案，然后将新解决方案及其分数添加到元提示中以进行下一个优化步骤。
- 元提示包含在整个优化过程中获得的解决方案分数对，以及任务的自然语言描述和（在提示优化中）任务中的一些示例。

## 🖥️OPRO: LLM AS THE OPTIMIZER

- 在每个优化步骤中，LLM 根据优化问题描述和元提示中先前评估的解决方案生成优化任务的候选解决方案。然后评估新的解决方案并将其添加到元提示中以进行后续优化过程。
- 权衡exploration和exploitation：LLM在研究更有希望区域的同时，也要积极探索新区域避免错过更好的解决方案。
- META-PROMPT DESIGN，meta-prompt包括以下两个部分：
    - Optimization problem description：对于优化问题的描述，可以包括目标函数和解决方案的约束。
    - Optimization trajectory：包括过去的解决方案及其优化分数，按升序排序。
