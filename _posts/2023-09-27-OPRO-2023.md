---
layout:     post
title:      OPRO			# 标题 
subtitle:   LARGE LANGUAGE MODELS AS OPTIMIZERS #副标题
date:       2023-09-27 				# 时间
author:     JiahaoLi 						# 作者
header-img: img/bolg-background.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Optimization
    - Prompt
    - LLMs
    - Agent
---

![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/OPRO_title.png)

## 📖 Abstract

- 文章提出Optimization by PROmpting (OPRO)，使用LLM作为优化器，在每个优化步骤中，LLM 根据包含先前生成的解决方案及其值的提示生成新的解决方案，然后对新解决方案进行评估并将其添加到下一个优化步骤的提示中。
- 通过实验证明OPRP优化的最佳prompt在GSM8K和Big-Bench任务上比人工设计的prompt效果更好。

## 👉Intro

![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/OPRO.png)

- OPRO优化目标是找到最大化任务准确性的提示。
- 将元提示作为输入，LLM 会生成目标函数的新解决方案，然后将新解决方案及其分数添加到元提示中以进行下一个优化步骤。
- 元提示包含在整个优化过程中获得的解决方案分数对，以及任务的自然语言描述和（在提示优化中）任务中的一些示例。

## 🖥️OPRO: LLM AS THE OPTIMIZER

- 在每个优化步骤中，LLM 根据优化问题描述和元提示中先前评估的解决方案生成优化任务的候选解决方案。然后评估新的解决方案并将其添加到元提示中以进行后续优化过程。
- 权衡exploration和exploitation：LLM在研究更有希望区域的同时，也要积极探索新区域避免错过更好的解决方案。
- META-PROMPT DESIGN，meta-prompt包括以下两个部分：
    - Optimization problem description：对于优化问题的描述，可以包括目标函数和解决方案的约束。
    - Optimization trajectory：包括过去的解决方案及其优化分数，按升序排序。

## 🧪Experiment
 - Evaluation Setup
     - Optimizer LLM: Pre-trained PaLM 2-L，instruction-tuned PaLM 2-L , text-bison, gpt-3.5-turbo, and gpt-4.
     - Scorer LLM: Pre-trained PaLM 2-L and text-bison
     - Benchmarks：GSM8K and Big-Bench Hard (BBH)
     - Implementation details：在每个优化步骤中，我们使用元提示提示优化器 LLM 8 次以生成 8 条指令，然后将这些指令及其训练分数添加到元提示中的优化轨迹中。我们每一步的元提示都包含迄今为止最好的 20 条指令以及从训练集中随机挑选的 3 个示例。

- Main results（GSM8K）

![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/OPRO_GSM8K.png)

    - 不同优化器LLM找到的指令风格差异很大：PaLM 2-L-IT和text-bison的指令很简洁，而GPT的指令又长又详细。
    - 尽管一些top instruction包含“逐步”短语，但大多数其他instruction通过不同的语义实现了可比较或更好的准确性。

- Main results（BBH）

![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/OPRO_BBH.png)

- Similar instruction——with the PaLM 2-L scorer on the GSM8K test set
    - “Let’s think step by step.” achieves accuracy 71.8
    - “Let’s solve the problem together.” has accuracy 60.5
    - “Let’s work together to solve this problem step by step.” is only 49.4

- 此外还做了消融实验，这里不过多介绍

## 📑Summary

- OPRO以LLM作为优化器，每一轮迭代优化中，LLM根据包含先前生成的解决方案及其值的提示生成新的解决方案，对新的解决方案进行评估后添加到下一轮迭代优化。

### 😉今天的分享就到这里啦，下次见咯！

