---
layout:     post
title:      ExpeL			# 标题 
subtitle:   LLM Agents Are Experiential Learners #副标题
date:       2023-09-25 				# 时间
author:     JiahaoLi 						# 作者
header-img: img/bolg-background.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Experience Learning
    - LLMs
    - agent
---

![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting@main/Img/Expel.png)

## 📖 Abstract

- 针对特定任务对其进行微调需要大量资源，并且可能会降低模型的泛化能力。
  
- 因此文章提出了一种新的经验学习智能体（ExpeL),允许从agent经验中学习而不需要参数更新。在推理时，智能体会回忆其提取的见解和过去的经验来做出明智的决策。

- 文章结果强调了 ExpeL 代理强大的学习功效，表明随着经验的积累，其性能持续增强。

## 👉Intro

- ExpeL agent通过反复试验自主地从一系列训练任务中收集经验。从这些经验中，它得出自然语言见解，并在测试期间采用自己的成功经验作为上下文示例。其强调在多项任务中保留经验以提高代理绩效的重要性，并且经验收集步骤不需要大量数据或人工标签。

- ExpeL分三个阶段运作
  - 将成功和失败的经验收集到一个经验池中。
  - 从这些经验中提取/抽象跨任务知识->我理解是将经验变为自己的见解或是一种对successful trajectories的学习。
  - 在评估任务中应用所获得的见解并回忆过去的成功。
 
## 🤖ExpeL

- Gathering Experiences
  - 每当agent完成任务或者达到最大步数时，ExpeL的经验池就会提取trajectory。如果agent在试验中成功，则代理将继续执行下一个任务。然而，如果agent失败，它将查看其失败的trajectory并进行自我反思，来了解下一次重试可以在哪些方面做的更好。

- Learning from Experiences
  - 给agent的instruction可以分解为任务规范和少数任务示例。可以利用agent从过去的经验中提取的见解来增强任务规范；对于少样本示例，我们可以允许代理从其经验池中检索前 k 个相关示例以帮助其决策。
  - Similar Experiences as Demonstrations：受到人类还会从记忆中回忆起他们在尝试执行该任务时解决过的类似任务作为参考的启发，文章提出提出经验回忆，根据任务相似性从训练期间收集的经验池中检索成功的轨迹。具体来说，ExpeL利用检索器和嵌入器获得任务相似度最高的前k个成功轨迹，如果智能体重复任何任务或执行与经验池中现有成功轨迹相似的任务，智能体只需紧密模仿成功轨迹即可，并且能力外推的负担较小。
  - Learning from Successes and Failures：他们通过两种方式让agent学习经验。首先，他们让agent将同一任务的失败轨迹与成功轨迹进行比较，以期望突出正确和错误的操作；其次，他们让代理识别来自不同任务的一组成功轨迹中的模式。
  - LLM可以通过“ADD”添加新的insight；“DOWNVOTE”否决现有见解；“UPVOTE”赞同现有见解；“EDIT”编辑现有见解。
  - 新添加的insight将具有与其关联的初始重要性计数 2，如果对其应用后续运算符 UPVOTE 或 EDIT，则计数将增加，而当对其应用 DOWNVOTE 时，计数将递减。如果insight的重要性计数达到零，它将被删除。
 
- Task Inference
  - 在agent收集经验并提取insight后，就可以执行新的任务并进行评估了。对于每个任务，任务规范（task specification）通过提取的insight列表增强，并且任务相似度最高的前 k 个轨迹将被检索并用作上下文中的例子。

## 🧪Experiment

- Setting
    - 实验基于四个基于文本的基准设计的：HotpotQA、ALF-World、WebShop and FEVER
    - 实验使用ReAct和Act作为baseline planning LLM agent

- Main Results
  - 与baseline agent相比，通过抽象洞察力和回忆成功轨迹的能力来增强agent可以提高所有环境中的性能。
  - Expel 在HotpotQA 中与 Reflexion 的性能相当(40% at R3 vs. 39%)，甚至在 ALFWorld 中的表现优于 Reflexion(54% at R3 vs. 59%)。
  - 在迁移学习中，使用 HotpotQA 数据集作为源任务，使用 FEVER 数据集作为目标任务。结果表明：从源域转移知识的两个代理都看到了性能提升。值得注意的是，具有一些上下文示例的agent比没有的有更显着的改进。

## 📑Summary

- ExpeL通过将成功和失败的经验收集到经验池，从这些经验中提取/抽象跨任务知识（insight），并在后续的任务中“回忆”insight来提高任务的成功率。

### 😉今天的分享就到这里啦，下次见咯
