---
layout:     post
title:      MEMWALKER		# æ ‡é¢˜ 
subtitle:   Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading   #å‰¯æ ‡é¢˜
date: Â  Â  Â  2023-10-28 				# æ—¶é—´
author:     JiahaoLi 						# ä½œè€…
header-img: img/bolg-background.jpg 	#è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true 						# æ˜¯å¦å½’æ¡£
tags:								#æ ‡ç­¾
    - LLMs
    - Memory  
---

## ğŸ“– Abstract

- Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once.
- However, this mechanism comes with a fundamental issue â€” the predetermined context window is bound to be limited.
- å°½ç®¡å°è¯•é€šè¿‡å¤–æ¨ä½ç½®åµŒå…¥ã€ä½¿ç”¨é€’å½’æˆ–é€‰æ‹©æ€§æ£€ç´¢é•¿åºåˆ—çš„é‡è¦éƒ¨åˆ†ç­‰æ–¹æ³•æ¥æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ï¼Œä½†é•¿æ–‡æœ¬ç†è§£ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚
- æ–‡ç« å¼•å…¥äº†MEMWALKERï¼Œè¿™æ˜¯ä¸€ç§é¦–å…ˆå°†é•¿ä¸Šä¸‹æ–‡å¤„ç†æˆæ‘˜è¦èŠ‚ç‚¹æ ‘çš„æ–¹æ³•ã€‚åœ¨æ¥æ”¶åˆ°æŸ¥è¯¢åï¼Œæ¨¡å‹å¯¼èˆªè¯¥æ ‘ä»¥æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶åœ¨æ”¶é›†åˆ°è¶³å¤Ÿä¿¡æ¯ååšå‡ºå“åº”ã€‚
- On long-text question answering tasks, MEMWALKER outperforms baseline approaches that use long context windows, recurrence, and retrieval.

## ğŸ§ Methods

- å…ˆè®¨è®ºä¸€ä¸‹å¾ªç¯æ¶æ„ï¼Œå¾ªç¯æ¶æ„å·²è¢«å¹¿æ³›ç ”ç©¶ä»¥è§£å†³é•¿åºåˆ—é—®é¢˜ï¼Œä»åŸºäºå¾ªç¯ç¥ç»ç½‘ç»œçš„æ¨¡å‹åˆ°åŸºäºç°ä»£å˜å‹å™¨çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ¯ä¸ªé‡å¤æ­¥éª¤éƒ½ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶ä¸”è®­ç»ƒç›®æ ‡ä¸ä¼šæŒ‡å¯¼å…³äºä¸‹æ¸¸ä»»åŠ¡çš„â€œå¦‚ä½•å‹ç¼©â€ã€‚é€šå¸¸ï¼Œè¿™ç§å‹ç¼©æ„å‘³ç€ä¸æœ€è¿‘çš„ä¿¡æ¯ç›¸æ¯”ï¼Œå¯¹æ—§åºåˆ—ä¿¡æ¯çš„å¬å›è¾ƒå¼±
![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting/Img/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-10-28%20213615.png)









