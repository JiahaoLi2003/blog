---
layout:     post
title:      MEMWALKER		# 标题 
subtitle:   Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading   #副标题
date:       2023-10-28 				# 时间
author:     JiahaoLi 						# 作者
header-img: img/bolg-background.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - LLMs
    - Memory  
---

## 📖 Abstract

- Large language models (LLMs) have advanced in large strides due to the effectiveness of the self-attention mechanism that processes and compares all tokens at once.
- However, this mechanism comes with a fundamental issue — the predetermined context window is bound to be limited.
- 尽管尝试通过外推位置嵌入、使用递归或选择性检索长序列的重要部分等方法来扩展上下文窗口，但长文本理解仍然是一个挑战。
- 文章引入了MEMWALKER，这是一种首先将长上下文处理成摘要节点树的方法。在接收到查询后，模型导航该树以搜索相关信息，并在收集到足够信息后做出响应。
- On long-text question answering tasks, MEMWALKER outperforms baseline approaches that use long context windows, recurrence, and retrieval.

## 🧐 Methods

- 先讨论一下循环架构，循环架构已被广泛研究以解决长序列问题，从基于循环神经网络的模型到基于现代变压器的模型。然而，每个重复步骤都会导致信息丢失，并且训练目标不会指导关于下游任务的“如何压缩”。通常，这种压缩意味着与最近的信息相比，对旧序列信息的召回较弱
![](https://cdn.jsdelivr.net/gh/JiahaoLi2003/ImgHosting/Img/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202023-10-28%20213615.png)









